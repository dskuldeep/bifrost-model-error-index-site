---
title: "The provided input does not match the expected type. Ensure that parameters like \"prompt\" are correctly formatted as strings."
provider: "openai"
provider_icon: "/logos/openai.png"
solved: true
slug: "input-type-error-9130"
---

## Raw Error Text

```text
{"input_variables": ["augmented_query"], "template": "Write a creative, engaging story that brings the scene to life. Describe the characters, setting, and actions in a way that would captivate a young audience the story must contains this \n            \n            Human: {augmented_query}\n            Chatbot:", "_type": "prompt"} is not of type "string" - "prompt"
```

## Reason

The `400 Bad Request` error indicates that the `prompt` field in the API request received a JSON object instead of the required string type. The raw error shows a structured object containing `input_variables` and `template`, which is the standard format for a LangChain `PromptTemplate` object.

Key reasons for this failure include:
1. **Invalid Request Structure**: The OpenAI API expects the `prompt` parameter to be a single string (for Completion models) or a `messages` array of objects (for Chat models). Providing a JSON object in the `prompt` field violates the API schema.
2. **Framework Integration Mismatch**: This error often occurs when a prompt template object is passed directly to an LLM provider's SDK or a gateway like Bifrost without first being converted into a string.
3. **Invalid or Malformed Input**: If input variables contain hidden characters or unexpected data types, the resulting payload may fail validation at the API gateway level.
4. **Model Parameter Constraints**: Certain parameters require strict type adherence. Passing configuration metadata (like `_type: "prompt"`) into a field reserved for the actual text input triggers a type mismatch error.

## Solution
To resolve the `400 Bad Request` error, you must ensure that the `prompt` parameter is sent as a plain text string. Follow these remediation steps:
1. **Format the Prompt Template**: If using a framework like LangChain, call the `.format()` or `.invoke()` method on your template before passing it to the model. This injects your variables and returns the final string.
2. **Verify Data Types**: Ensure that all parameters in your JSON payload match the expected types defined in the OpenAI API reference. The `prompt` field must be a string.
3. **Check API Request Structure**: Confirm you are using the correct endpoint. If you are using a Chat model (e.g., GPT-4o), you should be using the `messages` parameter (an array of objects) instead of the `prompt` parameter (a string).
4. **Validate Input Variables**: Inspect dynamic values like `augmented_query` to ensure they do not contain nested objects, unescaped special characters, or hidden formatting that could disrupt the JSON structure.
5. **Escape Special Characters**: If your prompt includes quotes or newlines, ensure they are properly escaped in the final JSON payload to maintain a valid string format.

## Suggested Links
- [https://platform.openai.com/docs/api-reference/completions/create](https://platform.openai.com/docs/api-reference/completions/create)
- [https://python.langchain.com/docs/concepts/#prompt-templates](https://python.langchain.com/docs/concepts/#prompt-templates)
- [https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create)
