---
title: "Unsupported parameter: 'temperature' is not supported with this model."
provider: "openai"
provider_icon: "/logos/openai.svg"
solved: true
slug: "unsupported-parameter-error-9507"
---

## Reason

The error "Unsupported parameter: 'temperature' is not supported with this model" occurs when an API request specifies a value for the `temperature` parameter that the selected model is not configured to handle. This behavior is most common with the OpenAI reasoning model series, including **o1-preview**, **o1-mini**, **o1**, and **o3-mini**.

Key reasons for this error include:
1. **Fixed Sampling for Reasoning Models**: OpenAI's reasoning models (the `o1` and `o3` series) currently require a fixed temperature of
1. Providing any other value (e.g., 0.7 or 0) will trigger a 400 Bad Request error.
2. **Parameter Restrictions**: Unlike GPT-4o or GPT-3.5, these models are optimized for a specific reasoning path where sampling variance is restricted to ensure consistency.
3. **Model-Specific Configurations**: Certain configurations within the Assistants API or specific fine-tuned deployments may also disable the ability to adjust temperature settings.
4. **Default Value Enforcement**: Even setting the temperature to 1 explicitly can occasionally cause issues in older SDK versions if the model expects the parameter to be omitted entirely.

## Solution
To resolve the error and ensure compatibility with models that have fixed sampling parameters, follow these steps:
1. **Use the Default Temperature**: If you are using an `o1` or `o3` series model, ensure the `temperature` is set to
1. Many developers find that explicitly passing `1` or `1.0` resolves the error while still satisfying API requirements.
2. **Omit the Parameter**: The most reliable way to avoid this error is to remove the `temperature` parameter from your API request entirely. When omitted, the API will automatically use the supported default value for that specific model.
3. **Verify Model Compatibility**: Check if the model you are targeting supports adjustable sampling. If your application logic requires varying temperatures (e.g., for creative writing), consider switching to a model like **gpt-4o** or **gpt-4o-mini**.
4. **Check Related Parameters**: Models that restrict `temperature` often also restrict other sampling parameters. Ensure you are not also passing non-default values for `top_p`, `presence_penalty`, or `frequency_penalty`, as these may cause similar "unsupported parameter" errors.
5. **Update SDKs**: Ensure you are using the latest version of the OpenAI Python or Node.js library, as newer versions are updated to handle the parameter requirements of the reasoning models more gracefully.

## Suggested Links
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
