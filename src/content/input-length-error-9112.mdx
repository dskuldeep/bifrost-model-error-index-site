---
title: "prompt is too long: 103078 tokens > 102398 maximum"
provider: "anthropic"
provider_icon: "/logos/anthropic.svg"
solved: true
slug: "input-length-error-9112"
---

## Reason

The HTTP 400 Bad Request error, specifically categorized as an `invalid_request_error`, occurs when an Anthropic API request exceeds the maximum allowed token limit for the context window. 

Key reasons for this error include:
1. **Token Limit Exceeded**: The specific message `prompt is too long: 103078 tokens > 102398 maximum` indicates that the cumulative token count of the request—including the system prompt, user messages, assistant conversation history, and tool definitions—has exceeded the model's capacity.
2. **Context Window Constraints**: Different Anthropic models have varying context windows (e.g., legacy Claude 2 models supported ~100k tokens, while Claude 3 and 3.5 models generally support 200k tokens). If a model is forced to process more than its hard limit, the API rejects the request.
3. **Request Content Structure**: This error is triggered during the validation phase of the request, meaning the format is correct but the content volume is mathematically invalid for the selected model's infrastructure.

## Solution
To resolve the "prompt is too long" error, you must ensure your input stays within the model's token constraints. Follow these steps:
1. **Trim the Prompt**: Manually or programmatically remove non-essential parts of the text to bring the token count below the maximum limit.
2. **Optimize Prompt Length**: 
   - Use clear, concise language to convey information.
   - Provide highly specific instructions to avoid unnecessary preamble.
   - Utilize system messages effectively to set global context rather than repeating it in every user turn.
3. **Implement Pre-flight Token Counting**: Use the official Anthropic `count_tokens` API endpoint or the `@anthropic-ai/sdk` to calculate the exact token count before sending the full request. This allows you to catch length issues client-side.
4. **Dynamic Trimming Logic**: Build a mechanism that automatically truncates the oldest parts of the conversation history or reduces the size of included documents when a certain token threshold (e.g., 95% of the limit) is reached.
5. **Retry with Adjusted Prompt**: If an automated process detects this error, configure a retry logic that summarizes the current context or trims the input and resubmits the request.

## Suggested Links
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
- [Redacted URL](Redacted URL)
