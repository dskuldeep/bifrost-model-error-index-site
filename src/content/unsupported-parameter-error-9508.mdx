---
title: "Unsupported parameter: 'temperature' is not supported with this model."
provider: "openai"
provider_icon: "/logos/openai.png"
solved: true
slug: "unsupported-parameter-error-9508"
---

## Reason

The error "Unsupported parameter: 'temperature' is not supported with this model" occurs when an API request specifies a value for the `temperature` parameter that the selected model is not configured to handle. This behavior is most common with the OpenAI reasoning model series, including **o1-preview**, **o1-mini**, **o1**, and **o3-mini**.

Key reasons for this error include:
1. **Fixed Sampling for Reasoning Models**: OpenAI's reasoning models (the `o1` and `o3` series) currently require a fixed temperature of
1. Providing any other value (e.g., 0.7 or 0) will trigger a 400 Bad Request error.
2. **Parameter Restrictions**: Unlike GPT-4o or GPT-3.5, these models are optimized for a specific reasoning path where sampling variance is restricted to ensure consistency.
3. **Model-Specific Configurations**: Certain configurations within the Assistants API or specific fine-tuned deployments may also disable the ability to adjust temperature settings.
4. **Default Value Enforcement**: Even setting the temperature to 1 explicitly can occasionally cause issues in older SDK versions if the model expects the parameter to be omitted entirely.

## Solution
To resolve the error and ensure compatibility with models that have fixed sampling parameters, follow these steps:
1. **Use the Default Temperature**: If you are using an `o1` or `o3` series model, ensure the `temperature` is set to
1. Many developers find that explicitly passing `1` or `1.0` resolves the error while still satisfying API requirements.
2. **Omit the Parameter**: The most reliable way to avoid this error is to remove the `temperature` parameter from your API request entirely. When omitted, the API will automatically use the supported default value for that specific model.
3. **Verify Model Compatibility**: Check if the model you are targeting supports adjustable sampling. If your application logic requires varying temperatures (e.g., for creative writing), consider switching to a model like **gpt-4o** or **gpt-4o-mini**.
4. **Check Related Parameters**: Models that restrict `temperature` often also restrict other sampling parameters. Ensure you are not also passing non-default values for `top_p`, `presence_penalty`, or `frequency_penalty`, as these may cause similar "unsupported parameter" errors.
5. **Update SDKs**: Ensure you are using the latest version of the OpenAI Python or Node.js library, as newer versions are updated to handle the parameter requirements of the reasoning models more gracefully.

## Suggested Links
- [https://github.com/jupyterlab/jupyter-ai/issues/994](https://github.com/jupyterlab/jupyter-ai/issues/994)
- [https://github.com/Nutlope/aicommits/issues/137](https://github.com/Nutlope/aicommits/issues/137)
- [https://community.openai.com/t/request-failed-with-status-code-400/39242](https://community.openai.com/t/request-failed-with-status-code-400/39242)
- [https://forum.bubble.io/t/openai-api-error-http-400/263917](https://forum.bubble.io/t/openai-api-error-http-400/263917)
- [https://community.openai.com/t/content-is-required-property-error-400/486260](https://community.openai.com/t/content-is-required-property-error-400/486260)
- [https://github.com/openai/openai-python/issues/1362](https://github.com/openai/openai-python/issues/1362)
- [https://community.studio3t.com/t/openai-implementation-is-incorrect-temperature-seems-to-be-set-at-1/1633](https://community.studio3t.com/t/openai-implementation-is-incorrect-temperature-seems-to-be-set-at-1/1633)
- [https://community.openai.com/t/cannot-set-temperature-parameter-in-assistant-api-call/721547](https://community.openai.com/t/cannot-set-temperature-parameter-in-assistant-api-call/721547)
- [https://platform.openai.com/docs/guides/reasoning/beta-limitations](https://platform.openai.com/docs/guides/reasoning/beta-limitations)
