---
title: "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead."
provider: "openai"
provider_icon: "/logos/openai.svg"
solved: true
slug: "parameter-error-9371"
---

## Reason

This error occurs because OpenAI has transitioned to a new parameter schema for its reasoning-based models. Specifically, the `max_tokens` parameter is no longer supported for the **o1 series** models (such as `o1`, `o1-preview`, and `o1-mini`).

The fundamental reasons for this change include:
1. **Reasoning Tokens:** The o1 series models generate internal "reasoning tokens" to process complex queries before producing a final response. These tokens are consumed during the generation process but are not visible in the final output.
2. **Total Generation Control:** The `max_tokens` parameter traditionally limited only the visible completion tokens. To provide developers with control over the entire generation process (including the hidden reasoning tokens), OpenAI introduced `max_completion_tokens`.
3. **Model Incompatibility:** While newer models require `max_completion_tokens` to manage their internal logic, older models like GPT-4o and GPT-4 still utilize the legacy `max_tokens` parameter, leading to inconsistencies if the incorrect parameter is used for the targeted model.

## Solution
To resolve this error, you must update your API request to use the parameter required by your specific model.

**For o1 Series Models (e.g., o1, o1-preview, o1-mini):**
1. Replace the `max_tokens` parameter with `max_completion_tokens` in your JSON payload or SDK configuration.
2. Ensure that the value provided for `max_completion_tokens` is high enough to accommodate both the internal reasoning tokens and the final visible response.

**For GPT-4 and GPT-3.5 Models (e.g., gpt-4o, gpt-4-turbo, gpt-3.5-turbo):**
1. Continue using the `max_tokens` parameter.
2. Do not use `max_completion_tokens` for these models, as it may not be compatible and could lead to further validation errors.

**Implementation Tip:**

If your application dynamically switches between models, implement a conditional check to apply the correct parameter based on the model name. For any model beginning with the `o1-` prefix, use `max_completion_tokens`; for others, default to `max_tokens`.

## Suggested Links
- [https://community.zapier.com/troubleshooting-99/chatgpt-error-400-max-token-is-too-large-32768-this-model-supports-at-most-4096-completion-tokens-39804](https://community.zapier.com/troubleshooting-99/chatgpt-error-400-max-token-is-too-large-32768-this-model-supports-at-most-4096-completion-tokens-39804)
- [https://github.com/spring-projects/spring-ai/issues/1411](https://github.com/spring-projects/spring-ai/issues/1411)
- [https://learn.microsoft.com/en-sg/answers/questions/2139738/openai-badrequesterror-error-code-400-((error-((me](https://learn.microsoft.com/en-sg/answers/questions/2139738/openai-badrequesterror-error-code-400-((error-((me)
- [https://community.openai.com/t/error-encountered-when-using-max-tokens-parameter-with-gpt-4-api/436386](https://community.openai.com/t/error-encountered-when-using-max-tokens-parameter-with-gpt-4-api/436386)
- [https://platform.openai.com/docs/guides/reasoning/advice-on-prompting](https://platform.openai.com/docs/guides/reasoning/advice-on-prompting)
- [https://community.make.com/t/issue-with-max-completion-tokens-in-openai-o1-models/56099](https://community.make.com/t/issue-with-max-completion-tokens-in-openai-o1-models/56099)
- [https://learn.microsoft.com/en-us/answers/questions/2139738/openai-badrequesterror-error-code-400-((error-((me](https://learn.microsoft.com/en-us/answers/questions/2139738/openai-badrequesterror-error-code-400-((error-((me)
- [https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_completion_tokens](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_completion_tokens)
